<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Diffusing Digits | Sean T. Howard</title> <meta name="author" content="Sean T. Howard"/> <meta name="description" content="Generating MNIST Digits from noise with HuggingFace Diffusers"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåå</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://seanhoward.me/blog/2022/Diffusing_Digits/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://seanhoward.me/"><span class="font-weight-bold">Sean</span> T. Howard</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Diffusing Digits</h1> <p class="post-meta">September 18, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> ¬† ¬∑ ¬† <a href="/blog/tag/PyTorch"> <i class="fas fa-hashtag fa-sm"></i> PyTorch</a> ¬† <a href="/blog/tag/Deep-Learning"> <i class="fas fa-hashtag fa-sm"></i> Deep-Learning</a> ¬† <a href="/blog/tag/Diffusion"> <i class="fas fa-hashtag fa-sm"></i> Diffusion</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/Intros"> <i class="fas fa-tag fa-sm"></i> Intros</a> ¬† </p> </header> <article class="post-content"> <h1 id="diffusing-digits---generating-mnist-digits-from-noise-with-huggingface-diffusers">Diffusing Digits - Generating MNIST Digits from noise with HuggingFace Diffusers</h1> <p><a href="https://colab.research.google.com/github/st-howard/blog-notebooks/blob/main/MNIST-Diffusion/Diffusion%20Digits%20-%20Generating%20MNIST%20Digits%20from%20noise%20with%20HuggingFace%20Diffusers.ipynb" target="_blank" rel="noopener noreferrer"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></p> <figure> <video width="480" height="360" controls=""> <source src="/assets/img/blogs/Diffusing_Digits_files/diffusion.mp4" type="video/mp4"></source> </video> <figcaption>Generating MNIST digits with diffusion</figcaption> </figure> <p>Diffusion models have become the state of the art generative model by learning how to progressively remove ‚Äúnoise‚Äù from a randomly generated noise field until the sample matches the training data distribution. Diffusion models are a fundamental part of several noteworthy text to image models, including Imagen, DALLE-2, and Stable Diffusion. However, they are capabilities beyond text to image generation and are applicable to a large variety of generative tasks.</p> <p>Here a minimal diffusion model is trained on the iconic <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener noreferrer">MNIST Digits</a> database using several <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">HuggingFace</a> libraries. The flow follows that of the <a href="https://github.com/huggingface/diffusers/blob/main/docs/source/training/overview.mdx" target="_blank" rel="noopener noreferrer">example</a> HuggingFace notebook for unconditional image generation. I chose HuggingFace libraries for the implementation to learn their framework and I found that they were a nice balance between coding everything up in raw PyTorch (as was done in <a href="https://huggingface.co/blog/annotated-diffusion" target="_blank" rel="noopener noreferrer">HuggingFace annotated diffusion blog post</a>) and tailored implementations such as Phil Wang‚Äôs <a href="https://github.com/lucidrains/denoising-diffusion-pytorch" target="_blank" rel="noopener noreferrer">denoising-diffusion-pytorch</a>.</p> <details> <summary> Diffusion Models - Quick Explanation </summary> <p>Conceptually, diffusion models are built upon a series of noising and denoising steps. In the noising process, random Gaussian noise is iteratively added to data (typically an image but can be any numeric datatype). After many steps of adding noise, the original data becomes indistinguishable from Gaussian noise. This noising process is going from <strong>right to left</strong> in the below figure from the <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">Denoising Diffusion Probabilistic Models paper</a> (often referred to as DDPM). In practice, getting from the original data to the step \(t\) of the noising process can be done in one go based upon convenient properties of Gaussians.</p> <figure> <img src="https://huggingface.co/blog/assets/78_annotated-diffusion/diffusion_figure.png" width="90%"> </figure> <p>The real juice of diffusion models is the denoising process. In the figure above, each denoising step (<strong>left to right</strong> in above figure), attempts to remove the noise added from previous step. Given noisy data, the diffusion model tries to predict the noise present in the data (slightly different to the above depiction which shows the model learning the conditional probability distribution \(p(x_{t-1} \vert x_t)\)). This noise is iteratively removed until the denoised data, which by characteristic of the training distribution, is left.</p> <p>Diffusion models can be broken down into two algorithms, one for training and one for sampling.</p> <h3 id="diffusion-models---training">Diffusion Models - Training</h3> <p>The training algorithm is relatively simple and follow the steps</p> <ul> <li>Take data from training distribution</li> <li>Randomly select a step within the noisig/denoising process</li> <li>Sample random Gaussian noise with zero mean and unit variance</li> <li>Take noise field and data from training distribution and noise it to selected step from noising process.</li> <li>Predict the noise present in the noisy data</li> <li>Update model based upon mean squared error of actual noise and predicted noise</li> </ul> <p>Which is shown in the psuedocode from the <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">Ho et. al paper</a>.</p> <figure> <img src="https://huggingface.co/blog/assets/78_annotated-diffusion/training.png" width="50%"> <figcaption></figcaption> </figure> <h3 id="diffusion-models---sampling">Diffusion Models - Sampling</h3> <p>With a model that takes a noisy image and predicts the noise given the step in the noising chain, can iteratively denoise the data with the following steps</p> <ul> <li>Generate the fully noised data at last step \(T\)</li> <li>For each step in the chain, predict the noise in the image and remove some fraction of it.</li> </ul> <p>Which is shown in the pseudocode</p> <figure> <img src="https://huggingface.co/blog/assets/78_annotated-diffusion/sampling.png" width="50%"> <figcaption></figcaption> </figure> <p>There are details about noise and learning rate schedules which were omitted from the above, but covered in the <a href="https://huggingface.co/blog/annotated-diffusion" target="_blank" rel="noopener noreferrer">annotated diffusion blog post</a></p> </details> <h2 id="outline">Outline</h2> <p>In creating a diffusion model with HuggingFace, I found there to be <strong>4</strong> main stages after choosing the hyperparameters, each with defined subtasks. I‚Äôve shown an outline below</p> <ol> <li> <a href="#defining-hyperparameters">Defining Hyperparameters</a> </li> <li> <a href="#preparing-mnist-dataset">Preparing Dataset</a> <ul> <li><a href="#downloading-mnist-with-huggingface-datasets">Downloading MNIST with HuggingFace <code class="language-plaintext highlighter-rouge">datasets</code></a></li> <li><a href="#data-preprocessing-and-augmentation">Data preprocessing and augmentation</a></li> </ul> </li> <li> <a href="#creating-the-diffusion-model">Creating the Diffusion Model</a> <ul> <li><a href="#u-net-for-mnist">U-Net for MNIST</a></li> <li><a href="#noise-scheduler">Noise Scheduler</a></li> <li><a href="#optimizer">Optimizer</a></li> <li><a href="#learning-rate-scheduler">Learning Rate Scheduler</a></li> </ul> </li> <li> <a href="#training-the-model">Training the Model</a> <ul> <li><a href="#working-with-memory-restrictions">Working with memory restrictions</a></li> <li><a href="#creating-and-running-training-loop">Creating and running training script</a></li> </ul> </li> <li><a href="#sample-some-good-looking-digits">Sampling Images</a></li> </ol> <details> <summary> Import libraries </summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Pytorch
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision</span>

<span class="c1"># HuggingFace
</span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="n">diffusers</span>
<span class="kn">import</span> <span class="n">accelerate</span>

<span class="c1"># Training and Visualization
</span><span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">PIL</span>
</code></pre></div> </div> </details> <h2 id="defining-hyperparameters">Defining Hyperparameters</h2> <p>In the training config class shown below, I‚Äôve chosen an image size of \(32 \times 32\) instead of the default MNIST resolution of \(28 \times 28\). This slight upscaling is in order to make the image width/height be a power of 2, i.e. \(2^5\). In the default U-Net architecture, each downsampling layer reduces the width and height of the image by 2. Therefore after the three downsampling blocks I used in the U-Net, the output size will be \(4 \times 4 \times N\), where \(N\) is a configurable parameter of the model architecture. As the width and height of the image is reduced, the number of learned channels increases. So in the U-Net configured here, the bottleneck layer has dimension of \(4 \times 4 \times 512\).</p> <p>The batch sizes chosen are done in order to comfortably fit on a 8 GB memory GPU. I find that training occupies approximately 4 GB of memory. Since one epoch contains all sixty thousand training examples, only a couple epochs are needed for the model to converge, with most of the learning being done within the first epoch.</p> <p>The <code class="language-plaintext highlighter-rouge">lr_warmup_steps</code> is the number of mini-batches where the learning rate is increased until hitting the base learning rate listed in <code class="language-plaintext highlighter-rouge">learning_rate</code>. After the learning rate reaches this value, a cosine scheduler is used to slowly decrease the learning rate, as described in <a href="https://arxiv.org/abs/2102.09672" target="_blank" rel="noopener noreferrer">Improved Denoising Diffusion Probabilistic Models</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">:</span>
    <span class="n">image_size</span><span class="o">=</span><span class="mi">32</span> <span class="c1">#Resize the digits to be a power of two
</span>    <span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">eval_batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">lr_warmpup_steps</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">mixed_precision</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fp16</span><span class="sh">'</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
    
<span class="n">config</span> <span class="o">=</span> <span class="nc">TrainingConfig</span><span class="p">()</span>
</code></pre></div></div> <h2 id="preparing-mnist-dataset">Preparing MNIST Dataset</h2> <h3 id="downloading-mnist-with-huggingface-datasets">Downloading MNIST with HuggingFace <code class="language-plaintext highlighter-rouge">datasets</code> </h3> <p>HuggingFace has almost ten thousand dataset for download, which can be searched from the <a href="https://huggingface.co/datasets" target="_blank" rel="noopener noreferrer">datasets tab</a> of their website. They can be downloaded with their <code class="language-plaintext highlighter-rouge">datasets</code> python library and the <a href="https://huggingface.co/docs/datasets/loading" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">load_dataset()</code></a> function.</p> <p>If not specified, the data will be downloaded to the <code class="language-plaintext highlighter-rouge">~/.cache</code> directory. If you want to put the files in another location, either specify the <code class="language-plaintext highlighter-rouge">data_dir</code> optional argument or change the environment variable <code class="language-plaintext highlighter-rouge">HF_DATASETS_CACHE</code> to the desired path.</p> <p>Here MNIST digits are loaded into a <code class="language-plaintext highlighter-rouge">Dataset</code> object, where metadata, labels, and images can be accessed in a manner similar to python dictionaries.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">mnist</span><span class="sh">'</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The dataset object is conveniently accessible with methods similar to a python dictionary</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_dataset</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dataset({
    features: ['image', 'label'],
    num_rows: 60000
})
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">].</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)).</span><span class="nf">show</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Image Size:</span><span class="sh">"</span><span class="p">,</span> <span class="n">mnist_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">].</span><span class="n">size</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Digit is labelled:</span><span class="sh">"</span><span class="p">,</span> <span class="n">mnist_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_0.png" width="15%"> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Image Size: (28, 28)
Digit is labelled: 5
</code></pre></div></div> <h3 id="data-preprocessing-and-augmentation">Data Preprocessing and Augmentation</h3> <p>As downloaded, the MNIST dataset contains 60,000 PIL images with pixel values in the range of \([0,255]\). The data must be scaled, resized, and turned into a tensor for ingestion by a PyTorch model. These transformations can be handled by torchvision‚Äôs transforms library. Transform objects can be sequentially listed in a Compose constructor, which will apply then apply the transformations when an image is passed as an argument.</p> <p>Three transforms are used. The first transforms the image to 32x32, in order to for the image width/height to be a power of two. The second transform turns the PIL image to a PyTorch tensor. When converting to a PyTorch tensor, the pixel range is transformed from \([0,255]\) to \([0,1]\). However, for the diffusion model the required pixel value range needs to be \([-1,1]\) since the Gaussian noise is zero mean, unit variance. Therefore, a lambda function is used to define a transform from \([0,1]\) to \([-1,1]\).</p> <p>The <code class="language-plaintext highlighter-rouge">Datasets</code> object has a method <code class="language-plaintext highlighter-rouge">set_transform()</code> which applies a function which takes the dataset object as an argument. Here the method is used to apply the torchvision transforms to the MNIST dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="n">preprocess</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span>
                <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">image_size</span><span class="p">)),</span>
            <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
            <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="nf">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">]]</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">images</span><span class="sh">"</span><span class="p">:</span> <span class="n">images</span><span class="p">}</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mnist_dataset</span><span class="p">.</span><span class="nf">reset_format</span><span class="p">()</span>
<span class="n">mnist_dataset</span><span class="p">.</span><span class="nf">set_transform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
</code></pre></div></div> <p>Once the dataset has been prepared with the proper transformers, it is ready to be passed directly into a PyTorch DataLoader.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
    <span class="n">mnist_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">train_batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="creating-the-diffusion-model">Creating the Diffusion Model</h2> <h3 id="u-net-for-mnist">U-Net for MNIST</h3> <p>The workhorse of the denoising diffusion model is a U-Net, which is predicts the noise present in the input image conditioned on the step in the noising process. HuggingFace‚Äôs Diffusers library has default a <a href="https://huggingface.co/docs/diffusers/api/models#diffusers.UNet2DModel" target="_blank" rel="noopener noreferrer">U-Net class</a> which creates a PyTorch model based upon the input values. Here the input and output channels are set to one since the image is black and white. The rest of the parameters mirror the choices found in the example notebook from HuggingFace.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">diffusers</span><span class="p">.</span><span class="nc">UNet2DModel</span><span class="p">(</span>
    <span class="n">sample_size</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">layers_per_block</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">block_out_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">512</span><span class="p">),</span>
    <span class="n">down_block_types</span><span class="o">=</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">DownBlock2D</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">DownBlock2D</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">AttnDownBlock2D</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">DownBlock2D</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">up_block_types</span><span class="o">=</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">UpBlock2D</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">AttnUpBlock2D</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">UpBlock2D</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">UpBlock2D</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div> <p>Check that the input image to the model and the output have the same shape</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_image</span> <span class="o">=</span> <span class="n">mnist_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">images</span><span class="sh">"</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">sample_image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input shape: torch.Size([1, 1, 32, 32])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Output shape:</span><span class="sh">'</span><span class="p">,</span> <span class="nf">model</span><span class="p">(</span><span class="n">sample_image</span><span class="p">,</span> <span class="n">timestep</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="sh">"</span><span class="s">sample</span><span class="sh">"</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output shape: torch.Size([1, 1, 32, 32])
</code></pre></div></div> <h3 id="noise-scheduler">Noise Scheduler</h3> <p>In diffusion models, the noise is added to images dependent on the step within noising/denoising process. In the original <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">DDPM paper</a>, the strength of the noise added to the image (i.e. the variance of the zero mean Gaussian) increased linearly with time steps. The Diffusers library has a <a href="https://huggingface.co/docs/diffusers/v0.3.0/en/api/schedulers#diffusers.DDPMScheduler" target="_blank" rel="noopener noreferrer">noise scheduler object</a> which handles the amount of noise to be added for a given step. The default values for noise are taken from the DDPM paper, but there are optional arguments to change the starting and ending noise strength, along with the how the noise changes with across steps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">noise_scheduler</span> <span class="o">=</span> <span class="n">diffusers</span><span class="p">.</span><span class="nc">DDPMScheduler</span><span class="p">(</span><span class="n">num_train_timesteps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">tensor_format</span><span class="o">=</span><span class="sh">'</span><span class="s">pt</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>We can take a digit and use the scheduler object to add noise. Below is the</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Original Digit</span><span class="sh">"</span><span class="p">)</span>
<span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToPILImage</span><span class="p">()(</span><span class="n">sample_image</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original Digit
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_1.png" width="15%"> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">sample_image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">([</span><span class="mi">199</span><span class="p">])</span>
<span class="n">noisy_image</span> <span class="o">=</span> <span class="n">noise_scheduler</span><span class="p">.</span><span class="nf">add_noise</span><span class="p">(</span><span class="n">sample_image</span><span class="p">,</span><span class="n">noise</span><span class="p">,</span><span class="n">timesteps</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Fully Noised Digit</span><span class="sh">"</span><span class="p">)</span>
<span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToPILImage</span><span class="p">()(</span><span class="n">noisy_image</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fully Noised Digit
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_2.png" width="15%"> </figure> <h3 id="optimizer">Optimizer</h3> <p>Let‚Äôs have the U-Net can learn with the <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">AdamW optimizer</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div> <h3 id="learning-rate-scheduler">Learning Rate Scheduler</h3> <p>As mentioned previously, in <a href="https://arxiv.org/abs/2102.09672" target="_blank" rel="noopener noreferrer">Improved Denoising Diffusion Probabilistic Models</a>, they find a learning rate schedule which first warmups for a fixed number of steps and then follows a cosine schedule thereafter to be effective in training the model. The diffusers library has a <a href="https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_cosine_schedule_with_warmup" target="_blank" rel="noopener noreferrer">method</a> which creates a PyTorch learning rate scheduler which follows the advice given in this paper.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Cosine learning rate scheduler
</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">diffusers</span><span class="p">.</span><span class="n">optimization</span><span class="p">.</span><span class="nf">get_cosine_schedule_with_warmup</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">lr_warmpup_steps</span><span class="p">,</span>
    <span class="n">num_training_steps</span><span class="o">=</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">num_epochs</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="training-the-model">Training the Model</h2> <h3 id="working-with-memory-restrictions">Working with memory restrictions</h3> <p>Running this on my local machine, I found that unless I set a limit on the VRAM accessible to PyTorch it would use it all up. This is good for maximizing utilization of a GPU cluster, but bad when iterating on a machine where the same GPU is rendering the operating system.</p> <p>To get around this, there is a useful <a href="https://pytorch.org/docs/stable/generated/torch.cuda.set_per_process_memory_fraction.html" target="_blank" rel="noopener noreferrer">cuda function</a> within PyTorch which sets the maximum fraction of total memory accessible. I‚Äôve set this to use 7 GB out of 8 GB, just so computer doesn‚Äôt come to a standstill.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_per_process_memory_fraction</span><span class="p">(</span><span class="mf">7.</span><span class="o">/</span><span class="mf">8.</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <h3 id="creating-and-running-training-loop">Creating and Running Training Loop</h3> <p>The training function first creates a HuggingFace <a href="https://huggingface.co/docs/accelerate/v0.12.0/en/package_reference/accelerator#accelerator" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">accelerator</code></a> object. The purpose of the <code class="language-plaintext highlighter-rouge">accelerator</code> object is to automatically handle device assignment for PyTorch objects when training on multiple devices and to make the code portable when running in multiple setups. Once created, the <code class="language-plaintext highlighter-rouge">accelerator</code> has a method <code class="language-plaintext highlighter-rouge">prepare</code> which takes all of the model/U-Net, optimizer, dataloader, and learning rate scheduler and automatically detects the correct device(s) and makes the appropriate <code class="language-plaintext highlighter-rouge">.to()</code> assignments.</p> <p>After those objects are ‚Äúprepared‚Äù, the training has an outer <code class="language-plaintext highlighter-rouge">for</code> loop for each epoch and an inner <code class="language-plaintext highlighter-rouge">for</code> loop for each mini-batch. In each mini-batch, a set of digits is taken from the dataset. Random noise with the same size of the minibatch is then sampled. Then, for each image in the minibatch, a random step in the noising process is (uniformly) selected. Noise is then added to each image based upon the randomly sampled noise and the randomly selected step. The U-Net then predicts the noise added to the image conditioned on the selected step. A mean squared error loss is then calculated between the predicted noise and the actual noise added to the image. This loss is then used to update the weights for each mini-batch.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_loop</span><span class="p">(</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">noise_scheduler</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">train_dataloader</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="p">):</span>

    <span class="n">accelerator</span> <span class="o">=</span> <span class="n">accelerate</span><span class="p">.</span><span class="nc">Accelerator</span><span class="p">(</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">mixed_precision</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="nf">prepare</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">lr_scheduler</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">progress_bar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">),</span>
                            <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">is_local_main_process</span><span class="p">)</span>
        <span class="n">progress_bar</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="n">clean_images</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="sh">'</span><span class="s">images</span><span class="sh">'</span><span class="p">]</span>

            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">clean_images</span><span class="p">.</span><span class="n">shape</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">clean_images</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">clean_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Sample a set of random time steps for each image in mini-batch
</span>            <span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="p">.</span><span class="n">num_train_timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">clean_images</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            
            <span class="n">noisy_images</span><span class="o">=</span><span class="n">noise_scheduler</span><span class="p">.</span><span class="nf">add_noise</span><span class="p">(</span><span class="n">clean_images</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>
            
            <span class="k">with</span> <span class="n">accelerator</span><span class="p">.</span><span class="nf">accumulate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
                <span class="n">noise_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">,</span><span class="n">timesteps</span><span class="p">)[</span><span class="sh">"</span><span class="s">sample</span><span class="sh">"</span><span class="p">]</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span><span class="n">noise</span><span class="p">)</span>
                <span class="n">accelerator</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                
                <span class="n">accelerator</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="mf">1.0</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
                <span class="n">lr_scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                
            <span class="n">progress_bar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">logs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">loss</span><span class="sh">"</span> <span class="p">:</span> <span class="n">loss</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">item</span><span class="p">(),</span>
                <span class="sh">"</span><span class="s">lr</span><span class="sh">"</span> <span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">.</span><span class="nf">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
            <span class="p">}</span>
            <span class="n">progress_bar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="o">**</span><span class="n">logs</span><span class="p">)</span>
    
    <span class="n">accelerator</span><span class="p">.</span><span class="nf">unwrap_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div> <p>Once the training loop set up, the function along with its arguments can be passed to the accelerate library‚Äôs <a href="https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/notebook#using-the-notebooklauncher" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">notebook launcher</code></a> to train within the notebook.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="p">)</span>

<span class="n">accelerate</span><span class="p">.</span><span class="nf">notebook_launcher</span><span class="p">(</span><span class="n">train_loop</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">num_processes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="create-a-sampling-function">Create a sampling function</h2> <p>Once the model has been trained, we can sample the model to create digits. Or more accurately create a sample which is within the learned distribution of the training samples, since some generated samples look like an alien‚Äôs numbering system, a mish-mash of the numbers 0-9.</p> <p>To sample images, the Diffusers library has several pipelines. However, <a href="https://github.com/huggingface/diffusers/issues/488" target="_blank" rel="noopener noreferrer">I found that these pipelines don‚Äôt work for single channel images</a> (<a href="https://github.com/huggingface/diffusers/pull/1025" target="_blank" rel="noopener noreferrer">which is now fixed!</a>). So I created a small function which samples the images, with an optional argument for saving off each step. Importantly, the function needs to have a <code class="language-plaintext highlighter-rouge">torch.no_grad()</code> decorator so the model doesn‚Äôt accumulate the history of the forward passes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">unet</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span><span class="n">seed</span><span class="p">,</span><span class="n">save_process_dir</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">save_process_dir</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">save_process_dir</span><span class="p">):</span>
            <span class="n">os</span><span class="p">.</span><span class="nf">mkdir</span><span class="p">(</span><span class="n">save_process_dir</span><span class="p">)</span>
    
    <span class="n">scheduler</span><span class="p">.</span><span class="nf">set_timesteps</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">image</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="nf">max</span><span class="p">(</span><span class="n">noise_scheduler</span><span class="p">.</span><span class="n">timesteps</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">noise_scheduler</span><span class="p">.</span><span class="n">timesteps</span><span class="p">:</span>
        <span class="n">model_output</span><span class="o">=</span><span class="nf">unet</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">t</span><span class="p">)[</span><span class="sh">'</span><span class="s">sample</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">image</span><span class="o">=</span><span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span><span class="nf">int</span><span class="p">(</span><span class="n">t</span><span class="p">),</span><span class="n">image</span><span class="p">,</span><span class="n">generator</span><span class="o">=</span><span class="bp">None</span><span class="p">)[</span><span class="sh">'</span><span class="s">prev_sample</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">save_process_dir</span><span class="p">:</span>
            <span class="n">save_image</span><span class="o">=</span><span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToPILImage</span><span class="p">()(</span><span class="n">image</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="n">save_image</span><span class="p">.</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">)).</span><span class="nf">save</span><span class="p">(</span>
                <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">save_process_dir</span><span class="p">,</span><span class="sh">"</span><span class="s">seed-</span><span class="sh">"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span><span class="o">+</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="o">+</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">num_steps</span><span class="o">-</span><span class="n">t</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span><span class="si">:</span><span class="mi">03</span><span class="n">d</span><span class="si">}</span><span class="sh">"</span><span class="o">+</span><span class="sh">"</span><span class="s">.png</span><span class="sh">"</span><span class="p">),</span><span class="nb">format</span><span class="o">=</span><span class="sh">"</span><span class="s">png</span><span class="sh">"</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToPILImage</span><span class="p">()(</span><span class="n">image</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div> <h2 id="sample-some-good-looking-digits">Sample some good looking digits!</h2> <p>Some samples look quit good‚Ä¶</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_image</span><span class="o">=</span><span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">noise_scheduler</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">test_image</span><span class="p">.</span><span class="nf">resize</span><span class="p">((</span><span class="mi">265</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_3.png" width="15%"> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_image</span><span class="o">=</span><span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">noise_scheduler</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">test_image</span><span class="p">.</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_4.png" width="15%"> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_image</span><span class="o">=</span><span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">noise_scheduler</span><span class="p">,</span><span class="mi">1991</span><span class="p">)</span>
<span class="n">test_image</span><span class="p">.</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_5.png" width="15%"> </figure> <p>But others aren‚Äôt quite recognizable as a number, but look like they <em>could</em> be number if history went slightly differently‚Ä¶</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_image</span><span class="o">=</span><span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">noise_scheduler</span><span class="p">,</span><span class="mi">2022</span><span class="p">)</span>
<span class="n">test_image</span><span class="p">.</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_6.png" width="15%"> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_image</span><span class="o">=</span><span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">noise_scheduler</span><span class="p">,</span><span class="mi">42</span><span class="p">)</span>
<span class="n">test_image</span><span class="p">.</span><span class="nf">resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">))</span>
</code></pre></div></div> <figure> <img src="/assets/img/blogs/Diffusing_Digits_files/Diffusion_Digits_7.png" width="15%"> </figure> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 Sean T. Howard. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>