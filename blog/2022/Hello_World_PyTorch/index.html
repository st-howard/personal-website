<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A 'Hello World' for PyTorch | Sean T. Howard</title> <meta name="author" content="Sean T. Howard"/> <meta name="description" content="A minimal neural network in PyTorch"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌌</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://seanhoward.me/blog/2022/Hello_World_PyTorch/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://seanhoward.me/"><span class="font-weight-bold">Sean</span> T. Howard</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">A 'Hello World' for PyTorch</h1> <p class="post-meta">September 7, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/PyTorch"> <i class="fas fa-hashtag fa-sm"></i> PyTorch</a>   <a href="/blog/tag/Deep-Learning"> <i class="fas fa-hashtag fa-sm"></i> Deep-Learning</a>     ·   <a href="/blog/category/Intros"> <i class="fas fa-tag fa-sm"></i> Intros</a>   </p> </header> <article class="post-content"> <p><a href="https://colab.research.google.com/github/st-howard/blog-notebooks/blob/main/Hello_World_PyTorch/Hello_World_PyTorch.ipynb" target="_blank" rel="noopener noreferrer"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></p> <p>PyTorch is a powerful machine learning library and is <a href="https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/" target="_blank" rel="noopener noreferrer">becoming the dominant deep learning framework</a>. I want to learn how to use PyTorch, so in the spirit of “Hello World”-like programs, the first thing I wanted to do is create a small neural network and train it on an easy dataset. The <a href="https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html" target="_blank" rel="noopener noreferrer">PyTorch tutorial</a> creates a fully connected neural network to train on FashionMNIST, but I want something even simpler so when things go wrong I’ll know that I’ve made a mistake somewhere.</p> <p>For this experiment, I’ll create a network of a few fully connected layers to classify the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html" target="_blank" rel="noopener noreferrer">two moons dataset</a>. I’ve chosen this dataset because the data is easily visualized and the optimal decision boundary is both non-linear and obvious on inspection.</p> <details> <summary> Load Libraries </summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Load libraries
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</code></pre></div> </div> </details> <details> <summary> Check for GPU </summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># check that GPU is recognized
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Does PyTorch recognize the GPU?</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">Yes</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">No</span><span class="sh">"</span><span class="p">)</span>
<span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
</code></pre></div> </div> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Does PyTorch recognize the GPU? Yes
</code></pre></div> </div> </details> <h2 id="the-dataset-two-moons">The Dataset: Two Moons</h2> <p>The first step is to investigate the toy dataset. The two moons dataset has one adjustable parameter, <code class="language-plaintext highlighter-rouge">noise</code>, which determines the spread of both half-moons. I’ve set <code class="language-plaintext highlighter-rouge">noise=0.1</code> so there are some values from each class that are very close at one end of each half moon. Even with this noise, the data appears separable such that a converged network should reach near 100% accuracy.</p> <details open=""> <summary> Generate Two Moons Dataset</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Generate the Two Moons dataset
</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span> <span class="o">=</span> <span class="nf">make_moons</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</code></pre></div> </div> </details> <details> <summary> Plot Two Moons Dataset </summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Plot the Two Moons dataset
</span><span class="n">class_colors</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y_data</span><span class="p">))):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
        <span class="n">x_data</span><span class="p">[</span><span class="n">y_data</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">x_data</span><span class="p">[</span><span class="n">y_data</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="n">class_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Class </span><span class="sh">"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Two Moons Dataset</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div> </div> </details> <figure> <img src="/assets/img/blogs/Hello_World_PyTorch_files/two_moons.png" width="90%"> <figcaption>The Two Moons toy dataset.</figcaption> </figure> <h2 id="construct-pytorch-model-and-dataset-for-training">Construct PyTorch Model and Dataset For Training</h2> <h3 id="make-network">Make Network</h3> <p>The simplest way to construct a neural network in PyTorch is with the <a href="https://pytorch.org/docs/master/generated/torch.nn.Sequential.html#torch.nn.Sequential" target="_blank" rel="noopener noreferrer">Sequential</a> container and <code class="language-plaintext highlighter-rouge">torch.nn</code> modules. This is one of <a href="https://h1ros.github.io/posts/3-ways-of-creating-a-neural-network-in-pytorch/" target="_blank" rel="noopener noreferrer">three ways</a> to construct a neural network in PyTorch. The network graph is defined by the order of the modules passed into <code class="language-plaintext highlighter-rouge">nn.Sequential</code>.</p> <p>The below model takes in the x and y values of two moons data set then has three fully connected hidden layers, with 20, 20, and 20 nodes respectively. A <a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html" target="_blank" rel="noopener noreferrer">ReLU</a> activation is used, followed by a <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html" target="_blank" rel="noopener noreferrer">one dimensional batch normalization</a>. The final output could either be a <a href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/" target="_blank" rel="noopener noreferrer">one-hot encoding</a> or a logistic regression to value where values greater than 0.5 are classified as 1 and values less than 0.5 are classified as 0 . I’ve chosen the logistic regression option.</p> <details open=""> <summary>Define PyTorch Network in Sequential Container</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Define a neural network
</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm1d</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of trainable parameters:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">))</span>
</code></pre></div> </div> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Sequential(
  (0): Linear(in_features=2, out_features=20, bias=True)
  (1): ReLU()
  (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): Linear(in_features=20, out_features=20, bias=True)
  (4): ReLU()
  (5): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): Linear(in_features=20, out_features=20, bias=True)
  (7): ReLU()
  (8): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (9): Linear(in_features=20, out_features=1, bias=True)
  (10): Sigmoid()
)
Number of trainable parameters: 1041
</code></pre></div> </div> </details> <h3 id="create-dataset-and-dataloader">Create Dataset and DataLoader</h3> <p>Next extend the <code class="language-plaintext highlighter-rouge">Dataset</code> class for the two moons dataset and create a <code class="language-plaintext highlighter-rouge">DataLoader</code> object. The extension of <code class="language-plaintext highlighter-rouge">Dataset</code> class must define a <code class="language-plaintext highlighter-rouge">__len__</code> function which returns the number of samples and a <code class="language-plaintext highlighter-rouge">__getitem__</code> function which returns an instance given an index. The <code class="language-plaintext highlighter-rouge">DataLoader</code> is a utility for iterating over the dataset. I’ve defined the minibatch size as 100, which is 10% of the 1000 instances of the two moons dataset. An enumerated <code class="language-plaintext highlighter-rouge">DataLoader</code> will provide one batch per iteration.</p> <details open=""> <summary>Create Dataset and DataLoader</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TwoMoons</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">x_data</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y_data</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">Y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span>
</code></pre></div> </div> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Create the training data as a DataLoader Object
</span><span class="n">two_moons</span> <span class="o">=</span> <span class="nc">TwoMoons</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">two_moons</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div> </div> </details> <h3 id="define-objective-function-and-optimizer">Define Objective Function and Optimizer</h3> <p>To train a network, a loss/objective function needs to be defined. This is the mathematical function which the network is trying to minimize. Since the above network is a regression problem, a mean squared error loss <code class="language-plaintext highlighter-rouge">MSELoss()</code> is an appropriate choice. This loss is minimized by the optimization algorithm, which takes the current predictions of the network and updates the model parameters. PyTorch has <a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener noreferrer">many optimization algorithms</a>, but I’ve used stochastic gradient descent or <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">SGD</code></a>.</p> <details open=""> <summary>Define Loss Function and Optimizer</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Define Loss Function and Optimizer
</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div> </div> </details> <h3 id="weight-initialization">Weight Initialization</h3> <p>Parameters in the network are initialized with reasonable defaults determined by PyTorch. For example, the <code class="language-plaintext highlighter-rouge">nn.Linear</code> module’s parameters are <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" target="_blank" rel="noopener noreferrer">initialized</a> from a uniform distribution (-$\sqrt{k}$,$\sqrt{k}$), where $k$ is the inverse of the number of input features to the layer. When training a network, the trainable parameters will change as the network learns. I found it helpful to be able to reset all the weights when experimenting, especially in combination with setting the default random seed. Resetting the network weights can be achieved by the <code class="language-plaintext highlighter-rouge">reset_parameters()</code> method for trainable modules (in this case the <code class="language-plaintext highlighter-rouge">nn.Linear</code> and <code class="language-plaintext highlighter-rouge">nn.BatchNorm1d</code> layers). By using the apply method of the <code class="language-plaintext highlighter-rouge">Sequential</code> container, the weights of each layer can be reset with below function:</p> <details open=""> <summary>Create helper function for resetting weights</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">reset_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">or</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
            <span class="n">m</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">reset_weights</span><span class="p">)</span>
</code></pre></div> </div> </details> <h2 id="train-the-model">Train the Model</h2> <p>With a network, loss function, optimization method, and a way to handle the data, we can begin to train the network. This is divided into two main steps, initialization and training.</p> <ol> <li>Initialization <ul> <li> <strong>Determine how long to train.</strong> In this case, I train for 10 epochs. Alternatively could train until certain loss is achieved. An epoch is a full pass through the training data. Based upon how I set up the <code class="language-plaintext highlighter-rouge">DataLoader</code>, one epoch consists of 10 mini-batches. In each mini-batch, a noisy estimate of the loss (noisy since only calculated on a fraction of the training data) is calculated and used to update the model weights. Many machine learning tasks are non-convex optimization problems, <a href="https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data" target="_blank" rel="noopener noreferrer">where a noisy estimate of the loss often outperforms the true loss</a> since it can escape local minima in the loss function.</li> <li> <strong>Set the random seed.</strong> When things go wrong, it is helpful to remove the RNG from what is happening. Each random seed will give different initial weights to the network and therefore different initial accuracy+decision boundary.</li> <li> <strong>Initial performance of Model.</strong> With weights initialized, test the accuracy of the model. On average expect this to be around 50% for binary classification, but will be off based on network architecture and initial weights.</li> </ul> </li> <li>Training <ul> <li> <strong>Zero the gradients.</strong> By default the <a href="https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html" target="_blank" rel="noopener noreferrer">gradients are accumulated in a buffer</a>, so need to zero them out so back propagation is calculated correctly.</li> <li> <strong>Make prediction and calculate loss.</strong> Network output is calculated by passing the input values directly to the model object. These predictions and the true values are then passed to the loss function, giving the loss for the mini-batch.</li> <li> <strong>Back Propagation.</strong> With a loss, we can calculate the gradient of the network with <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">.backward()</code></a>. After the gradient is calculated, the optimizer can update the weights of the network with the <a href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">.step()</code></a> method.</li> </ul> </li> </ol> <details open=""> <summary>Initialize and Train the Model</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># 1. Initialization
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="nf">init_weights</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">loss_history</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_epochs</span><span class="p">))</span>
<span class="n">accuracy_history</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_epochs</span><span class="p">))</span>

<span class="n">initial_preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">two_moons</span><span class="p">.</span><span class="n">X</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span>
<span class="n">initial_loss</span> <span class="o">=</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">initial_preds</span><span class="p">,</span> <span class="n">two_moons</span><span class="p">.</span><span class="n">Y</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">float</span><span class="p">())</span>
<span class="n">initial_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">initial_preds</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">).</span><span class="nf">round</span><span class="p">().</span><span class="nf">bool</span><span class="p">(),</span> <span class="n">two_moons</span><span class="p">.</span><span class="n">Y</span><span class="p">.</span><span class="nf">bool</span><span class="p">()).</span><span class="nf">sum</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Initial Loss: </span><span class="si">{</span><span class="n">initial_loss</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Initial Accuracy: </span><span class="si">{</span><span class="n">initial_acc</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">two_moons</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#2. Learning
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_samples</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_data</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">float</span><span class="p">())</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
        <span class="n">num_correct</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">predictions</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">round</span><span class="p">().</span><span class="nf">bool</span><span class="p">(),</span>
                                <span class="n">targets</span><span class="p">.</span><span class="nf">bool</span><span class="p">()).</span><span class="nf">sum</span><span class="p">()</span>
        <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">loss_history</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_loss</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">accuracy_history</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_correct</span><span class="o">/</span><span class="n">total_samples</span>

    <span class="nf">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">02</span><span class="n">d</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s"> | Loss: </span><span class="si">{</span><span class="n">loss_history</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> </span><span class="se">\t</span><span class="s"> | Accuracy: </span><span class="si">{</span><span class="n">accuracy_history</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Initial Loss: 0.25068870186805725
Initial Accuracy: 0.5560000538825989 

Epoch: 01/10 | Loss: 0.09092 	 | Accuracy: 0.88900
Epoch: 02/10 | Loss: 0.01654 	 | Accuracy: 0.98800
Epoch: 03/10 | Loss: 0.00792 	 | Accuracy: 0.99200
Epoch: 04/10 | Loss: 0.00350 	 | Accuracy: 0.99700
Epoch: 05/10 | Loss: 0.00244 	 | Accuracy: 0.99800
Epoch: 06/10 | Loss: 0.00178 	 | Accuracy: 0.99800
Epoch: 07/10 | Loss: 0.00155 	 | Accuracy: 1.00000
Epoch: 08/10 | Loss: 0.00136 	 | Accuracy: 0.99900
Epoch: 09/10 | Loss: 0.00119 	 | Accuracy: 1.00000
Epoch: 10/10 | Loss: 0.00133 	 | Accuracy: 0.99800
</code></pre></div> </div> </details> <h2 id="inspect-model-results">Inspect Model Results</h2> <p>In addition to training, I’ve kept track of the model accuracy (correct predictions) and the model loss (mean squared error) across each epoch. The network converges to an optimal solution very quickly, reaching 98.8% accuracy after just two epochs.</p> <details> <summary>Plot Loss and Accuracy Curves</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Plot the Loss History and the Accuracy
# Plot the Loss and Accuracy History
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">.</span><span class="nf">twinx</span><span class="p">()</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span> <span class="n">loss_history</span><span class="p">,</span> <span class="sh">'</span><span class="s">g-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span> <span class="n">accuracy_history</span><span class="p">,</span>
         <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">MSE Loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</code></pre></div> </div> </details> <figure> <img src="/assets/img/blogs/Hello_World_PyTorch_files/history.png" width="90%"> <figcaption>Loss and Accuracy During Training</figcaption> </figure> <p>Additionally, by making predictions for a grid of points, we can visualize the decision boundary learned by the network. I’ve found that the decision boundary between regions where there is training data is similar across random seeds. However, the decision boundary outside of this distribution changes based on the random seed/initial state of the network. This highlights a (potential) weakness of neural nets, as <a href="https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html" target="_blank" rel="noopener noreferrer">they can perform poorly on data out of their training distribution</a>.</p> <details> <summary>Plot Decision Boundary</summary> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Plot the scatter plot and the decision boundary
</span>
<span class="n">x0_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x1_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">pred_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">x0_grid</span><span class="p">,</span> <span class="n">x1_grid</span><span class="p">)))</span>
<span class="n">pred_grid</span> <span class="o">=</span> <span class="n">pred_grid</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">pred_array</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">pred_grid</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">pred_grid</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">pred_grid</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">makeGridPrediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pred_array</span><span class="p">):</span>
    <span class="n">model_array_preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">pred_array</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span>

    <span class="n">model_grid_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flip</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">model_array_preds</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="p">(</span><span class="n">pred_grid</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pred_grid</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="p">),</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model_grid_preds</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span>
    <span class="nf">makeGridPrediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pred_array</span><span class="p">),</span>
    <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="n">x0_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x0_grid</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x1_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x1_grid</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
    <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">seismic_r</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Plot the data again
</span><span class="n">class_colors</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y_data</span><span class="p">))):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
        <span class="n">x_data</span><span class="p">[</span><span class="n">y_data</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">x_data</span><span class="p">[</span><span class="n">y_data</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="n">class_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Class </span><span class="sh">"</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Two Moons Dataset - Learned Decision Boundary</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div> </div> </details> <figure> <img src="/assets/img/blogs/Hello_World_PyTorch_files/boundary.png" width="90%"> <figcaption>The learned decision boundary of network.</figcaption> </figure> <h4 id="check-network-evolution">Check Network Evolution</h4> <p>The evolution of the network can be visualized for this toy dataset by plotting the decision boundary for each mini-batch. Also, the impact of random weight initialization can be seen by the two different decision boundaries for the first mini-batch.</p> <figure> <video width="480" height="360" controls=""> <source src="/assets/img/blogs/Hello_World_PyTorch_files/learning_seed42.mp4" type="video/mp4"></source> </video> <figcaption>Network Evolution - Random Seed = 42</figcaption> </figure> <figure> <video width="480" height="360" controls=""> <source src="/assets/img/blogs/Hello_World_PyTorch_files/learning_seed2022.mp4" type="video/mp4"></source> </video> <figcaption>Network Evolution - Random Seed = 2022</figcaption> </figure> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Sean T. Howard. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>