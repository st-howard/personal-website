<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Discount DALL-E | Sean T. Howard</title> <meta name="author" content="Sean T. Howard"/> <meta name="description" content="exploring text to image methods"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌌</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://seanhoward.me/projects/discount_dalle/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://seanhoward.me/"><span class="font-weight-bold">Sean</span> T. Howard</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Discount DALL-E</h1> <p class="post-description">exploring text to image methods</p> </header> <article> <p><em>Images below are generated with code from the <a href="https://github.com/CompVis/latent-diffusion" target="_blank" rel="noopener noreferrer">latent-diffusion</a> repository, weights pre-trained on the <a href="https://laion.ai/blog/laion-400-open-dataset/" target="_blank" rel="noopener noreferrer">LAION-400M</a> dataset</em></p> <h2 id="prompt"><strong>Prompt:</strong></h2> <h5 id="subject"> <em>Subject</em>:</h5> <p><select class="form-select form-select-lg mb-3" id="Context_dropdown" style="width:auto;"><option value="cat">A cat wearing sunglasses</option></select></p> <h5 id="style"> <em>Style</em>:</h5> <p><select class="form-select form-select-lg mb-3" id="Style_dropdown" style="width:auto;"><option value="van_gogh">In the style of Van Gogh, Cezanne, Cubism</option></select></p> <div class="container"> <div class="row row-cols-4 no-gutters" id="ai_grid"> </div> </div> <p><br></p> <h2 id="intro">Intro</h2> <p>Text to image models have made significant progress recently, with Google’s <a href="https://imagen.research.google/" target="_blank" rel="noopener noreferrer">Imagen</a> (<a href="https://arxiv.org/abs/2205.11487" target="_blank" rel="noopener noreferrer">paper</a>) and <a href="https://parti.research.google/" target="_blank" rel="noopener noreferrer">Parti</a> (<a href="https://arxiv.org/abs/2206.10789" target="_blank" rel="noopener noreferrer">paper</a>), and Open AI’s <a href="https://openai.com/dall-e-2/" target="_blank" rel="noopener noreferrer">DALLE-2</a> (<a href="https://arxiv.org/abs/2204.06125" target="_blank" rel="noopener noreferrer">paper</a>) achieving start of the art results. Open AI and startups like <a href="https://www.midjourney.com" target="_blank" rel="noopener noreferrer">Midjourney</a> are creating tiered services for generating images from paying customer prompts. Open source implementations of smaller models with limited compute are available in online web apps, notably <a href="https://www.craiyon.com/" target="_blank" rel="noopener noreferrer">DALLE-mini/craiyon</a>. Top performing models contain billions of parameters, trained on hundreds of millions to billions of images. The complexity, compute, and cost associated with these models suggest that they are beyond the reach of the individual. However, publicly available weights make results a step below state of the art possible for free on a personal device. <strong>This is my attempt to run and explore text to image models locally, on my personal desktop computer</strong>.</p> <h2 id="how-text-to-image-generation-works">How Text to Image Generation Works</h2> <p>Improvements to text to image generation have been driven by the adaptation of a neural network architecture known as transformers to the domain of image processing and synthesis. <a href="https://arxiv.org/abs/1706.03762?context=cs" target="_blank" rel="noopener noreferrer">Originally</a> applied to natural language processing (NLP) tasks, transformers learn complex interrelationships between elements in a sequence such as text. Transformers consist of either an encoder, decoder, or both. The encoder transforms data into a fixed length sequence, which roughly corresponds to the algorithm’s understanding of what it is seeing in a learned feature space. For the example of translation, the encoder takes a segment of text, say in English, and generates a representation of that text that is language agnostic. The second part of a transformer, a decoder, takes a sequence and then learns the mapping of said sequence to an output. In the translation example, it learns how to take the language agnostic representation of a phrase to a different language, such as French. A nice illustrated introduction into transformers can be found <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">here</a>.</p> <h5 id="vision-transformer">Vision Transformer</h5> <p>A key feature of transformers which makes them difficult to use for image data is self-attention. In self-attention, pairwise relationships between elements of a sequence are learned. Or in other words, one element in the sequence <em>attends</em> to another element in same sequence, thus the name self-attention. For images, the number of elements in a sequence describing all pixel values is very large (\(M \times N \times C\) plus a positional encoding where the image has \(MxN\) pixels and \(C\) channels). The <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer">breakthrough</a> in applying transformers to images was to divide the image into digestablie chunks to create smaller, manageable sequences, as shown below (from <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" target="_blank" rel="noopener noreferrer">Google’s AI blog</a>).</p> <figure> <img src="/assets/img/ai_art/ViT.gif" width="60%" alt="ViT"> <figcaption>The Vision Transformer (ViT)</figcaption> </figure> <p>In this paper introducing vision transformers (ViT), the authors noted that convolutional neural networks actually outperformed transformers when the size of the training dataset was small, and transformers only gained the advantag after training on very large training datasets. This is due to convolutional nueral networks having a <em>strong inductive bias</em>. What does this mean? Convolutional neural networks convolve small matrices/tensors known as kernels to extract important features from an image. While these kernels are learned over time, using these kernels for convolutions tells the algorithm, from an architectural perspective, that local interactions are most important. Conversely, transformers inherently treat all interactions across the image equally to start, with no preference for local features. Therefore, convolution neural nets get a figurative head start on transformers since local features such as edges are very important. However, the transformer architecture can eventually learn non-local interactions better than convolutional architectures, given enough training data. Which is why the ViT architecture beat then state of the art algorithms based on recurrent convolutional neural networks, given enough training data.</p> <h5 id="vqgan">VQGAN</h5> <p>These tradeoffs are described in the introduction of the <a href="https://arxiv.org/abs/2012.09841" target="_blank" rel="noopener noreferrer"><em>Taming Transformers for Hig-Resolution Image Synthesis</em></a> paper</p> <blockquote> <p>“In contrast to the predominant vision architecture, convolutional neural networks (CNNs), the transformer architecture contains no built-in inductive prior on the locality of interactions and is therefore free to learn complex relationships among its inputs. However, this generality also implies that it has to learn all relationships, whereas CNNs have been designed to exploit prior knowledge about strong local correlations within images.”</p> </blockquote> <p>Within this paper, they use the best parts of both convoluational and transformer architectures</p> <blockquote> <p>“We hypothesize that low-level image structure is well described by a local connectivity, i.e. a convolutional architecture, whereas this structural assumption ceases to be effective on higher semantic levels … Our key insight to obtain a … model is that, <em>taken together, convolutional and transformer architectures can model the compositional nature of our visual world</em>”</p> </blockquote> <p>and propose a model architecture called <em>VQGAN</em>, shown below. At a high level, the VQGAN first encodes local features with a CNN into a latent space \(\hat{z}\), or a space of learned features. Then a transformer is applied to the latent space, which takes the latent space representation of the image, \(\hat{z}\), and then learns a sequence representation \(\mathbf{s}\) (positionally encoded to \(z_q\)) by predicting the next element of the sequence given the previous elements. This process of predicting the next sequence element from the previous elements is known as an <em>autoregressive</em> model. The positionally encoded sequence is then decoded by the CNN decoded and a resulting image is formed. If this process is done in the absence of a prompt, it is called <em>undconditioned</em>. If a text prompt is given, the predicition of the next element of the sequence is conditional on the text prompt and the model is <em>conditioned</em>.</p> <figure> <img src="/assets/img/ai_art/VQGAN.png" width="90%"> <figcaption>Schematic of VQGAN in Image Synthesis. From <a href="https://arxiv.org/abs/2012.09841" target="_blank" rel="noopener noreferrer"> Taming Transformers for Hig-Resolution Image Synthesis.</a></figcaption> </figure> <h5 id="latent-diffusion">Latent Diffusion</h5> <p>An alternative to the autoregressive approach is a process known as <em>diffusion</em>. In diffusion, an image is first turned into random noise and then “denoised” by attempting to reverse the steps. A schematic of this processes is shown below</p> <figure> <img src="/assets/img/ai_art/diffusion_example.png" width="90%"> <figcaption>Schematic of Diffusion Models. From <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/" target="_blank" rel="noopener noreferrer">the Assembly AI blog.</a> </figcaption> </figure> <p>The same <a href="https://github.com/CompVis" target="_blank" rel="noopener noreferrer">research lab</a> which came up with VQGANs described above, where the transformer is applied to a latent space, applied diffusion to the latent space in the paper <a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreferrer"><em>High-Resolution Synthesis with Latent Diffusion Models</em></a>. The benefit of applying diffusion models to the latent (feature) space is that the high level semantic concepts are efficiently compressed, compared to previous diffusion models acting on the pixel space. The model can then learn to create a representation of the image within the latent space from pure noise. Like the VQGAN, the denoising process tries to reproduce a sequence which matches the training data distribution (unconditioned) or matches an expected distribution based on the encoding given to the model (conditioned). Text to image generation is a conditioned version of this process, while the generation of celebrity faces described in the paper is unconditioned.</p> <p>Fortunately, the latent diffusion model is both computationally light (so I can run it on my personal computer) and has publicly available weights from training on captioned images on the internet. So this is the model I used to create the images at the top of the page. However, latent-diffusion models are more powerful than just text to image generation. The latent diffusion paper also illustrated impressive results for inpainting, semantic image generation, and super resolution.</p> <h5 id="comparing-top-models">Comparing Top Models</h5> <p>The latest text to image generation paper, <a href="https://arxiv.org/abs/2206.10789" target="_blank" rel="noopener noreferrer">Google’s Parti</a>, compares respective models and their architectures</p> <figure> <img src="/assets/img/ai_art/parti_comparisons.png" width="50%"> <figcaption>Comparison of top text to image models. From <a href="https://arxiv.org/abs/2206.10789" target="_blank" rel="noopener noreferrer">the Parti Paper.</a></figcaption> </figure> <p>which shows that state of the art performance can be achieved with both autoregressive and diffusion based models. The distinguishing feature between models of the same class is mainly the encoding of images and text. For example, DALLE-2 uses <a href="https://openai.com/blog/clip/" target="_blank" rel="noopener noreferrer">CLIP</a> to encode text and images to respective latent spaces. CLIP then learns how to go from a text encoding to an image encoding, which is used in their diffusion model to generate an image. Alternatively, Google’s Imagen uses their pre-trained and frozen (not updated during training) <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" target="_blank" rel="noopener noreferrer">T5-XXL</a> encoder for text embedding. One particularly interesting result is the increase in performance with model size in Google’s Parti, shown below</p> <p style="text-align:center;"> <img src="/assets/img/ai_art/parti_parameters.png" width="80%"> </p> <p>suggesting that better performance can be achieved with increasingly ginormous models. Resistance to overfitting and better performance with model sized has also been seen in the best performing text models, like GPT 3 and BLOOM, each with around 175 billion parameters. The model the <a href="https://github.com/CompVis/latent-diffusion#text-to-image" target="_blank" rel="noopener noreferrer">latent-diffusion</a> repo provided and the one I used to generate images had 1.45 billion parameters. Which is fortunate for me, since this is near the limit of the model size I can hold in my GPU’s VRAM.</p> <h2 id="image-generation-takeaways">Image Generation Takeaways</h2> <h4 id="1-supremacy-of-top-models">1. Supremacy of Top Models</h4> <p>Both Google’s Imagen and Parti provide separate lists of text prompts to be used as a benchmark for future models. In a similar way, I can compare prompts they used to the latent diffusion model I used. When using the prompts <em>“A brain riding a rocketship heading towards the moon”</em> (Imagen) and <em>“An astronaut riding a horse in a photorealistic style”</em> (DALLE-2), it is obvious that their models are superior. This likely due to a combination of a better training set, better model architecture (specifically encoding and decoding of text and images), and more parameters in their models.</p> <p style="text-align:center;"> <img src="/assets/img/ai_art/top_model_comps.png" width="90%"> </p> <h4 id="2-trial-and-error">2. Trial and Error</h4> <p>In text to image generation, you’ll often have an scene in mind and properly generating that scene with text is challenging. This challenge has lead to the niche study of “<strong>prompt engineering</strong>”, which studies what words or phrases elicit desired images from the model. For the latent diffusion model I used, more complex subjects (like the brain riding the rocketship shown above) did not produce great results. However, describing a subject <em>in the style of X</em> did yield good results, which is why I formatted the images at the top of the page as a selection of a subject then a style. When exploring what prompts to use, I found this <a href="https://dallery.gallery/the-dalle-2-prompt-book/" target="_blank" rel="noopener noreferrer"><em>DALLE-2 prompt book</em></a> had a variety of nice examples.</p> <p>Another feature of text to image generation is that the models sample the latent space of the text prompt, generating many unique images for the same prompt. This naturally leads to a distribution in the quality of images generated from a prompt. The most impressive images are those shown in press releases, papers, and company websites. These individually selected images have been deemed <em>cherry-picked</em> by the DALLE-2 user community. The images shown at the top of the page are not cherry picked, though I have selected prompts I thought produced decent images.</p> <h4 id="3-model-see-model-do">3. Model See Model Do</h4> <p>A drawback to the publicly available LAION dataset is the presence of stock photos and cropped images, which can be seen in watermarks and white bars appearing in generated images. In the images generated by state of the art models, at least their published results, don’t have these deleterious features. It is unclear whether this comes from superior discrimination in their model or a cleaner training dataset. However, the distribution of watermarks is not equal across prompts. For example, I found watermarks were more likely on prompts describing pencil drawings than those in the style of Van Gogh.</p> <p style="text-align:center;"> <img src="/assets/img/ai_art/watermark_examples.png" width="90%"> </p> <h2 id="running-locally">Running Locally</h2> <p>Here are the steps I used to create the images at the top of the page.</p> <p>With <code class="language-plaintext highlighter-rouge">conda</code> installed</p> <h4 id="1-clone-the-latent-diffusion-repo">1. Clone the latent-diffusion repo</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/CompVis/latent-diffusion.git
</code></pre></div></div> <h4 id="2-create-the-conda-virtual-environment">2. Create the conda virtual environment</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd latent-diffusion
conda env create -f environment.yaml
conda activate ldm
</code></pre></div></div> <p>Note: Need to install the appropriate version of pytorch. If you have an Nvidia GPU, the CUDA version. Otherwise, the CPU version is needed. If using the CPU version, then the <code class="language-plaintext highlighter-rouge">txt2img.py</code> script will need to be adjusted since it contains code specifically for CUDA. I was lucky to have an Nvidia GPU, so the code worked out of the box. Info for downloading a specific configuration of Pytorch can be found <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">here</a>.</p> <h4 id="3-download-the-pretrained-weights">3. Download the pretrained weights</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir -p models/ldm/text2img-large/
wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt
</code></pre></div></div> <h4 id="4-run-the-script">4. Run the script!</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python scripts/txt2img.py --prompt "Your prompt here!"
</code></pre></div></div> <p>The image outputs will be default 256x256 and appear in the outputs folder local to the repo. Look into the <code class="language-plaintext highlighter-rouge">txt2img.py</code> script to see the adjustable parameters. For the above images, I set the diffusion dimensions to 250, which was the level of diminishing returns mentioned on the latent-diffusion README. I also used the <code class="language-plaintext highlighter-rouge">--scale</code> set to 7.5. The scale is related to “Classifier-free guidance”, which is a relative weighting to conditioned image generation (conditioned in this case is the text prompt) and unconditioned image generation in the sampling process.</p> <h4 id="5-optional-upsample-the-image-using-ersgan">5. (Optional) Upsample the image using ERSGAN</h4> <p>Since I was limited by VRAM, I upscaled the images from 256x256 to 512x512 using ERSGAN.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/xinntao/Real-ESRGAN.git
cd Real-ESRGAN
wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models
python inference_realesrgan.py -n RealESRGAN_x4plus -i inputs --face_enhance
</code></pre></div></div> <script src="https://seanhoward.me/assets/js/populate_dropdown.js"></script> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Sean T. Howard. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>